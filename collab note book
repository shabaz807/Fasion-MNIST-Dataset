import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
 
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import FashionMNIST
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from torchvision.utils import make_grid
 
import os
from tqdm.notebook import tqdm
import warnings
 
warnings.filterwarnings('ignore')

transform = transforms.Compose([
                                transforms.ToTensor(),
                                transforms.Normalize(mean=0,std=1)
])
train_dataset = FashionMNIST(train=True,transform=transform,root=os.getcwd(),download=True)
test_dataset = FashionMNIST(train=False,transform=transform,root=os.getcwd(),download=True)

print("Train dataset:",train_dataset)
print("Test dataset:",test_dataset)

train_loader = DataLoader(train_dataset,
                          batch_size=60,
                          shuffle=True,
                          num_workers=4)
test_loader = DataLoader(test_dataset,
                          batch_size=60,
                          shuffle=False,
                          num_workers=4)
                          
train_iterator = iter(train_loader)

train_batch = next(train_iterator)

for item in train_batch:
  print(item.size())
  
def plot_images(batch):
  img_grid = make_grid(batch[0], nrow=4)
  img_np = img_grid.numpy()
  img_np = np.transpose(img_np , (1,2,0))

  plt.figure(figsize=(8,8))
  plt.imshow(img_np)
  plt.title(batch[1])
  #plt.xtickets([])
  #plt.ytickets([])
  plt.plot()
  
plot_images(train_batch)

flatten = nn.Flatten()
flatten(train_batch[0]).size()

model = nn.Sequential(
    nn.Flatten(),
    #1st hidden layer
    nn.Linear(in_features=784,out_features=512),
    nn.ReLU(),
    #2st hidden layer
    nn.Linear(in_features=512,out_features=512),
    nn.ReLU(),
    #3st hidden layer
    nn.Linear(in_features=512,out_features=256),
    nn.ReLU(),
    #out layer
    nn.Linear(in_features=256,out_features=10),
    nn.Softmax(dim=1)
)
model

pred_batch = model(train_batch[0])
print(pred_batch.size())

pred_labels = torch.argmax(pred_batch,axis=1)
print(pred_labels)

loss_criterion = nn.CrossEntropyLoss()

loss_criterion(pred_batch,train_batch[1])

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

def evaluate_on_gpu(model,dataloader):
  with torch.no_grad():
    total,correct = 0,0

    for batch in dataloader:
      images,labels = batch
      images,labels = images.to(device),labels.to(device)

      out_probs = model(images)
      out_labels = torch.argmax(out_probs,axis=1)
      total +=labels.size(0)
      correct +=torch.sum(labels == out_labels).item()

  return 100 * correct/total

def train_on_gpu(model,loss_criterion,optimizer,train_loader,test_loader,epochs=250):
  hist = {'loss':[],
          'train_acc':[],
          'test_acc':[]}
  for epoch_num in tqdm(range(1,epochs+1), desc='Training', total=epochs):
    losses = []
    for batch in train_loader:
      images,labels = batch
      images,labels = images.to(device),labels.to(device)

      optimizer.zero_grad()
      out = model(images)

      loss = loss_criterion(out,labels)
      losses.append(loss.item())
      loss.backward()

      optimizer.step()

    hist['loss'].append(np.array(losses).mean())
    train_acc = evaluate_on_gpu(model,train_loader)
    test_acc = evaluate_on_gpu(model,test_loader)
    hist['train_acc'].append(train_acc)
    hist['test_acc'].append(test_acc)
 
  fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(14,7))
  ax[0].plot(range(1,epochs+1),hist['loss'],label='loss')
  ax[0].grid()
  ax[0].set_xlabel('Epochs')
  ax[0].set_ylabel('Loss value')
  ax[0].set_title('Epochs vs Loss')
 
  ax[1].plot(range(1,epochs+1),hist['train_acc'],'b-',label='Training accuracy')
  ax[1].plot(range(1,epochs+1),hist['test_acc'],'m-',label='Test accuracy')
  ax[1].grid()
  ax[1].set_xlabel('Epochs')
  ax[1].set_ylabel('Loss value')
  ax[1].set_title('Epochs vs Loss')
  plt.show()
  return model

model = nn.Sequential(
    nn.Flatten(),
    #1st hidden layer
    nn.Linear(in_features=784,out_features=650),
    nn.ReLU(),
    #2nd hidden layer
    nn.Linear(in_features=650,out_features=420),
    nn.ReLU(),
    #3st hidden layer
    nn.Linear(in_features=420,out_features=360),
    nn.ReLU(),
    #4st hidden layer
    nn.Linear(in_features=360,out_features=230),
    nn.ReLU(),
    #output layer
    nn.Linear(in_features=230,out_features=10),
    nn.Softmax(dim=1)
)

model = model.to(device)

loss_criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

model = train_on_gpu(model,loss_criterion,optimizer,train_loader,test_loader)

